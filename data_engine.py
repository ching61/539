import requests
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any
import pandas as pd
import urllib3

# Disable SSL warnings globally as per the original script's approach
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class DailyCashCrawler:
    """ä»Šå½©539è³‡æ–™çˆ¬èŸ²é¡åˆ¥"""
    
    def __init__(self):
        self.base_url = "https://api.taiwanlottery.com/TLCAPIWeB/Lottery"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.verify = False # Ignore SSL certificate verification
    
    def crawl_daily_cash(self, year_month: str, page_num: int = 1, page_size: int = 50) -> Dict[str, Any]:
        """çˆ¬å–ä»Šå½©539è³‡æ–™"""
        url = f"{self.base_url}/Daily539Result"
        params = {
            'month': year_month,
            'pageNum': page_num,
            'pageSize': page_size
        }
        
        try:
            print(f"æ­£åœ¨çˆ¬å–ä»Šå½©539è³‡æ–™: {year_month}, é æ•¸: {page_num}")
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            if data.get('rtCode') == 0:
                return data.get('content', {})
            else:
                print(f"API éŒ¯èª¤: {data.get('rtMsg', 'æœªçŸ¥éŒ¯èª¤')}")
                return {}
                
        except requests.exceptions.RequestException as e:
            print(f"è«‹æ±‚ä»Šå½©539è³‡æ–™å¤±æ•—: {e}")
            return {}
        except json.JSONDecodeError as e:
            print(f"è§£æä»Šå½©539 JSON å¤±æ•—: {e}")
            return {}
    
    def process_daily_cash_data(self, raw_data: List[Dict]) -> List[Dict]:
        """è™•ç†ä»Šå½©539è³‡æ–™æ ¼å¼ï¼Œè¿”å›åˆ—è¡¨ä»¥ä¾¿æ–¼è½‰æ›ç‚ºDataFrame"""
        processed_list = []
        
        for item in raw_data:
            try:
                lottery_date_str = item['lotteryDate'].replace('T00:00:00', '')
                lottery_date = datetime.fromisoformat(lottery_date_str)
                tw_year = lottery_date.year - 1911
                
                period = str(item['period'])
                
                draw_numbers = item['drawNumberAppear']
                numbers = sorted(draw_numbers[:5])
                
                processed_list.append({
                    'draw': period,
                    'date': f"{tw_year}/{lottery_date.month:02d}/{lottery_date.day:02d}",
                    'ad_date': lottery_date.strftime('%Y-%m-%d'), # For sorting and internal use
                    'numbers': ','.join(f"{n:02d}" for n in numbers), # Format as "01,02,03,04,05"
                    'price': 8000000,
                    'lottery_type': 'daily_cash'
                })
                
            except (KeyError, ValueError, IndexError) as e:
                print(f"è™•ç†ä»Šå½©539è³‡æ–™éŒ¯èª¤: {e}, é …ç›®: {item}")
                continue
        
        return processed_list
    
    def get_existing_data_df(self, filepath: str) -> pd.DataFrame:
        """è®€å–ç¾æœ‰CSVè³‡æ–™ç‚ºDataFrame"""
        if os.path.exists(filepath):
            try:
                df = pd.read_csv(filepath, dtype={'draw': str}) # Ensure 'draw' is read as string
                # Convert 'ad_date' back to datetime for proper comparison
                df['ad_date'] = pd.to_datetime(df['ad_date'])
                return df
            except Exception as e:
                print(f"è®€å–ç¾æœ‰CSVè³‡æ–™å¤±æ•—: {e}")
                return pd.DataFrame()
        return pd.DataFrame()
    
    def get_latest_ad_date(self, df: pd.DataFrame) -> datetime:
        """å¾DataFrameä¸­æ‰¾åˆ°æœ€æ–°æ—¥æœŸ (AD calendar)"""
        if not df.empty and 'ad_date' in df.columns:
            return df['ad_date'].max().to_pydatetime() # Convert Timestamp to datetime.datetime
        return None
    
    def crawl_and_save_daily_cash(self, start_year: int = 2014, start_month: int = 1):
        """æ™ºèƒ½çˆ¬å–ä»Šå½©539è³‡æ–™ä¸¦å„²å­˜åˆ°CSV"""
        output_dir = 'lottery_data'
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, 'lottery_data.csv')
        
        existing_df = self.get_existing_data_df(filepath)
        latest_date_ad = self.get_latest_ad_date(existing_df)
        
        current_date = datetime.now()
        
        if latest_date_ad:
            # å¾æœ€æ–°è³‡æ–™çš„æœˆä»½é–‹å§‹çˆ¬å–ï¼ˆç¢ºä¿ä¸éºæ¼ï¼‰
            start_crawl_date = latest_date_ad.replace(day=1)
            print(f"ğŸ“Š æª¢æ¸¬åˆ°ç¾æœ‰è³‡æ–™ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date_ad.strftime('%Y-%m-%d')}")
            print(f"ğŸ¯ å¢é‡æ›´æ–°ï¼šå¾ {start_crawl_date.strftime('%Y-%m')} é–‹å§‹çˆ¬å–æ–°è³‡æ–™")
        else:
            start_crawl_date = datetime(start_year, start_month, 1)
            print(f"ğŸ†• é¦–æ¬¡çˆ¬å–ä»Šå½©539è³‡æ–™ï¼Œå¾ {start_year}-{start_month:02d} é–‹å§‹")
        
        all_new_records = []
        new_count = 0
        
        current_month_iter = start_crawl_date
        while current_month_iter <= current_date:
            year_month = f"{current_month_iter.year}-{current_month_iter.month:02d}"
            
            try:
                raw_content = self.crawl_daily_cash(year_month)
                if raw_content and 'daily539Res' in raw_content:
                    processed_list = self.process_daily_cash_data(raw_content['daily539Res'])
                    
                    for record in processed_list:
                        # Only add if it's newer than the latest existing record
                        # Or if there's no existing data
                        if latest_date_ad is None or datetime.fromisoformat(record['ad_date']) > latest_date_ad:
                            all_new_records.append(record)
                            new_count += 1
                    
                    if processed_list: # Only print if there was data for the month
                        print(f"âœ… {year_month}: è™•ç†äº† {len(processed_list)} ç­†è³‡æ–™")
                
                time.sleep(1) # Avoid frequent requests
                
            except Exception as e:
                print(f"âŒ çˆ¬å– {year_month} å¤±æ•—: {e}")
            
            # Move to the next month
            if current_month_iter.month == 12:
                current_month_iter = current_month_iter.replace(year=current_month_iter.year + 1, month=1)
            else:
                current_month_iter = current_month_iter.replace(month=current_month_iter.month + 1)
        
        if all_new_records:
            new_df = pd.DataFrame(all_new_records)
            
            # Combine existing and new data, remove duplicates based on 'draw' and sort by 'ad_date'
            combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset=['draw'])
            
            # CRITICAL FIX: Ensure 'ad_date' column is uniformly datetime before sorting
            combined_df['ad_date'] = pd.to_datetime(combined_df['ad_date'])
            
            # Now sort
            combined_df = combined_df.sort_values(by='ad_date').reset_index(drop=True)
            
            # Save to CSV
            combined_df.to_csv(filepath, index=False, encoding='utf-8')
            print(f"\nğŸ‰ ä»Šå½©539è³‡æ–™æ›´æ–°å®Œæˆï¼")
            print(f"ğŸ“ˆ æœ¬æ¬¡æ–°å¢ {new_count} ç­†è¨˜éŒ„ï¼Œç¸½è¨ˆ {len(combined_df)} ç­†")
            print(f"âœ… è³‡æ–™å·²å„²å­˜åˆ°: {filepath}")
        elif not existing_df.empty:
            print(f"\nğŸ‰ ä»Šå½©539è³‡æ–™å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°ã€‚ç¸½è¨ˆ {len(existing_df)} ç­†è¨˜éŒ„")
        else:
            print(f"\nâš ï¸ æœªèƒ½ç²å–ä»»ä½•ä»Šå½©539è³‡æ–™ã€‚")

def main():
    crawler = DailyCashCrawler()
    crawler.crawl_and_save_daily_cash()

if __name__ == "__main__":
    main()
